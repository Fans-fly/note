分布式id

    uuid(uuid有5个版本)
        版本1: 时间+mac地址
        版本2: 对版本1进行了改造,使用了dce算法,保证mac不被泄露
        版本3: 基于字符的MD5算法, hash, 会有hash冲突
        版本4: 随机的UUID  大概几百年左右会重复1次
        版本5: 基于sha1算法
    java.util.UUID类中使用了2个版本的UUID
        版本3: MD5
        版本4: 是个16进制,128位二进制转换成的32位的16进制,然后加上4个"-",一共是36位.   8位-4位-Mmmm-4-12, 中的M代表版本号
    机器码+数据库自增
    数据库自增(步长)
    redis 自增(redis是命令执行单线程,无需考虑并发),还可以使用incre + 时间戳拼接
    zk/mongdb都和redis差不多, 但是费钱,搭redis是要机器的.
    雪花算法: 64位二进制   1bit(符号位) + 41位的时间戳(毫秒时间  系统时间 - 系统上线时间) + 10位的机器id(唯一标志) + 12位的序列号(递增)
        该算法最大的坑就是要注意避免分库分表id不均匀的问题,比如生成的id都是偶数结尾的,那肯定不行
    改造版:leaf-agement  leaf-snowflake
    美团的leaf, 百度uidgenerator
    本质:唯一id大部分场景都是拼接的,才能保证唯一
什么时候需要分库分表？

    在阿里巴巴公布的开发手册中，建议MySQL单表记录如果达到500W这个级别，或者单表容量达到2GB，一般就建议进行分库分表。而考虑到分库分表需要对数据
        进行再平衡，所以如果要使用分库分表，就要在系统设计之初就详细考虑好分库分表的方案，这里要分两种情况。
    一般对于用户数据这一类后期增长比较缓慢的数据，一般可以按照三年左右的业务量来预估使用人数，按照标准预设好分库分表的方案。
    而对于业务数据这一类增长快速且稳定的数据，一般则需要按照预估量的两倍左右预设分库分表方案。并且由于分库分表的后期扩容是非常麻烦的，所以在进行分
        库分表时，尽量根据情况，多分一些表。最好是计算一下数据增量，永远不用增加更多的表
    另外，在设计分库分表方案时，要尽量兼顾业务场景和数据分布。在支持业务场景的前提下，尽量保证数据能够分得更均匀。
    最后，一旦用到了分库分表，就会表现为对数据查询业务的灵活性有一定的影响，例如如果按userId进行分片，那按age来进行查询，就必然会增加很多麻烦。如
        果再要进行排序、分页、聚合等操作，很容易就扛不住了。这时候，都要尽量在分 库分表的同时，再补充设计一个降级方案，例如将数据转存一份到ES，ES可以实现
        更灵活的大数据聚合查询。

1、分库分表形式

    水平分库:建立结构一样的库，根据分库方案划分数据到不同库
    水平分表:建立多个表，根据分表方案划分数据到不同表
    垂直分库:根据业务模块，把一组表划分为一个库
    垂直分表:根据字段的活跃性，把字段组拆分到不同的表中

分库分表包含分库和分表 两个部分，而这两个部分可以统称为数据分片,从分拆的角度上，可以分为垂直分片和水平分片

    垂直分片：核心理念就是专库专用。按照业务将表进行归类，分布到不同的数据库或表中，从而将压力分散至不同的数据库或表。
        垂直分片可以缓解数据量和访问量带来的问题，但无法根治决单点数据库的性能瓶颈。垂直拆分即拆表字段;
    水平分片：通过某个字段(或某几个字段)，根据某种规则（如hash取模、range）将数据分散至多个库或表中, 同样类型的数据放在不同的表中;
        range划分利于数据迁移，但是存在数据热点问题。hash取模划分，不会存在明显的热点问题，但是不利于扩容。可以range+hash取模结合使用。
        水平分片从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案
    常用的分片策略有：
        取余\取模 ： 优点 均匀存放数据，缺点 扩容非常麻烦
        按照范围分片 ： 比较好扩容， 数据分布不够均匀
        按照时间分片 ： 比较容易将热点数据区分出来。
        按照枚举值分片 ： 例如按地区分片
        按照目标字段前缀指定进行分区：自定义业务规则分片
    一般来说，在系统设计阶段就应该根据业务耦合松紧来确定垂直分库，垂直分表方案，在数据量及访问压力不是特别大的情况，首先考虑缓存、读写分离、索引技
        术等方案。若数据量极大，且持续增长，再考虑水平分库水平分表方案


2、分库分表解决方案

    分库分表工具（轻量级）
        1、sharding-sphere ：jar 包，前身是 sharding-jdbc （当当）
        2、TSharding ： 蘑菇街的中间件
    分库分表工具（重量级）
        1、Mycat ： 中间件产品，基于开源的 cobar 开发
        2、TDDL Smart Client ： jar 包，Taobao Distribute Data Layer
        3、Atlas ： 奇虎 360
        4、alibaba.cobar ： 阿里 B2B 部门开发
        5、Oceanus ： 58 同城的数据库中间件
        6、OneProxy ： 支付宝首席架构师楼方鑫开发
        7、vitess ： 谷歌开发的数据库中间件

    
3、分库分表中存在的问题
    
    1、事务问题
        方案一：使用分布式事务
            优点：交由数据库管理，简单有效
            缺点：性能代价高，特别是shard越来越多时
        方案二：由应用程序和数据库共同控制
            原理：将一个跨多个数据库的分布式事务分拆成多个仅处 于单个数据库上面的小事务，并通过应用程序来总控 各个小事务。
            优点：性能上有优势
            缺点：需要应用程序在事务控制上做灵活设计。如果使用了spring的事务管理，改动起来会面临一定的困难。
    
    2、跨节点的 Join 问题
        1.只要是进行切分，跨节点 Join 的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。
            在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。
        2.借助搜索引擎
    
    3、跨节点聚合问题
        比如 count、order by、group by 等聚合函数问题，方案是各节点完成计算后，交由业务层进行合并
        多节点的查询可以是并行的，因此大多数情况他比单一大表快很多，但是如果结果集很大，可能会导致内存消耗过高
    
    4、数据迁移，容量规划，扩容等问题
        这些问题目前都没有特别好的解决方案，每个方案都或多或少的有一些问题存在，因此这个问题的解决难度其实挺高的
    
    5、ID重复问题
        数据被切分后，就不能依赖数据库的自增 ID 进行赋值，另外 ID 还需要承担携带路由信息的功能，以降低查询难度
        一种方案是使用 UUID ，但是 UUID 比较长会占用较多的存储空间，另外一方面，UUID 对索引不友好
        一种方案是通过维护一个 ID 签发表来对 ID 进行签发，但是这会导致插入需要增加一次查询，且该表容易成为性能瓶颈存在单点故障问题
        一种方案是使用雪花算法进行 ID 的下发
    
    6、跨分片的排序问题
        如果排序字段是分片字段，则可以直接使用分片排序
        如果排序字段不是分片字段，则需要先在分片上进行排序，然后到业务系统进行合并，然后再排序
    
    7、分库策略、分库数量
        这个需要根据实际的业务场景，进行合理的分配，否则容易给后期造成很大的问题

    8、数据倾斜问题(有些表的数据多,有些表的数据少)
        参考:https://developer.aliyun.com/article/797071
        我们之前的项目可以修改哪些: 根据项目id进行hash分表,但是有些表的数据已经比较多了,那么这个时候我们可以调整项目id,当项目id分到某些表的时候,再重新随机生成.
        但是针对不能修改分表key的就很尴尬了. 只能将该表再进行分开,需要做如下步骤
            1.比如根据用户id分表,原来是分到表A的,但是表A的数据已经很多了,可以再对id进行另一种算法,排除表A再重新分表.
            2.需要将原来的表A里的数据,根据新的分表算法,进行重新分配.

    9、公共表处理
        实际的应用场景中，参数表、数据字典表等都是数据量较小，变动少，而且属于高频联合查询的依赖表。这一类表一般就需要在每个数据库中都保存一份，并且所有
        对公共表的操作都要分发到所有的分库去执行。


